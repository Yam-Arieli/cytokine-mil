{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Cytokine Signaling Dynamics — 10-Cytokine Subset Experiment\n",
    "\n",
    "Runs the full AB-MIL pipeline on a curated 10-cytokine subset + PBS control.\n",
    "\n",
    "## Rationale for cytokine selection\n",
    "\n",
    "The subset is designed to provide a **clear expected contrast** between easy-to-learn\n",
    "(high learnability AUC) and hard-to-learn (low AUC) cytokines, grounded in both\n",
    "the literature and preliminary full-experiment rankings reported in the source paper.\n",
    "\n",
    "The hard group includes IL-12 as a **cascade positive control**: the paper (Fig. 4i)\n",
    "explicitly shows that IL-12 drives its PBMC effects indirectly via IFN-γ induction,\n",
    "making it a clean test of whether the model recovers secondary-cascade cytokines as\n",
    "harder to learn than direct activators.\n",
    "\n",
    "### Expected easy group — direct, PBMC-specific, strong 24-h transcriptional response\n",
    "| Cytokine | Key mechanism | Evidence |\n",
    "|---|---|---|\n",
    "| IL-4 | Strong Th2/B cell axis via STAT6 | Ranked #1 in full-experiment preliminary run |\n",
    "| M-CSF | Direct monocyte survival/activation via CSF1R | Ranked #3 in preliminary run |\n",
    "| IL-10 | Strong monocyte/B cell response via STAT3 | Ranked #4 in preliminary run |\n",
    "| TNF-alpha | Canonical early NF-κB in monocytes/T cells | Well-established rapid inflammatory activator |\n",
    "| IL-2 | Direct T/NK activation via γc receptor | Ranked #10 in preliminary run |\n",
    "\n",
    "### Expected hard group — indirect, non-PBMC targets, or cascade-dependent\n",
    "| Cytokine | Key mechanism | Why hard |\n",
    "|---|---|---|\n",
    "| IL-22 | Epithelial STAT3 via IL-22R1 | Targets epithelium, not PBMCs — minimal direct PBMC signal |\n",
    "| VEGF | Endothelial growth via VEGFR | Endothelial target; PBMCs lack significant VEGFR expression |\n",
    "| IL-12 | IFN-γ induction in NK/T cells | **Cascade control**: effect is indirect (Fig. 4i explicitly shows secondary IFN-γ cascade) |\n",
    "| OSM | Oncostatin M via gp130 | Non-PBMC primary targets; ranked near bottom in preliminary run |\n",
    "| HGF | Hepatocyte growth via c-Met | Hepatocyte/stromal target; minimal direct PBMC transcriptional response |\n",
    "\n",
    "**Pre-registered predictions:**\n",
    "- IL-4, M-CSF, IL-10 should rank in the top 3 by learnability AUC.\n",
    "- IL-22, VEGF, HGF should rank in the bottom 3.\n",
    "- IL-12 should rank low despite being immunostimulatory (indirect cascade effect, not direct PBMC activation).\n",
    "\n",
    "Connect to the cluster kernel before running.\n",
    "All paths in `configs/default.yaml` point to cluster storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from cytokine_mil.data.label_encoder import CytokineLabel\n",
    "from cytokine_mil.data.dataset import PseudoTubeDataset, CellDataset\n",
    "from cytokine_mil.models.instance_encoder import InstanceEncoder\n",
    "from cytokine_mil.models.attention import AttentionModule\n",
    "from cytokine_mil.models.bag_classifier import BagClassifier\n",
    "from cytokine_mil.models.cytokine_abmil import CytokineABMIL\n",
    "from cytokine_mil.training.train_encoder import train_encoder\n",
    "from cytokine_mil.training.train_mil import train_mil\n",
    "from cytokine_mil.experiment_setup import (\n",
    "    build_stage1_manifest,\n",
    "    filter_manifest,\n",
    "    split_manifest_by_donor,\n",
    "    build_encoder,\n",
    "    build_mil_model,\n",
    ")\n",
    "from cytokine_mil.analysis.dynamics import (\n",
    "    aggregate_to_donor_level,\n",
    "    rank_cytokines_by_learnability,\n",
    "    compute_cytokine_entropy_summary,\n",
    "    compute_confusion_entropy_summary,\n",
    "    build_cell_type_confidence_matrix,\n",
    ")\n",
    "from cytokine_mil.analysis.validation import (\n",
    "    check_seed_stability,\n",
    "    check_functional_groupings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "with open(\"cytokines/cytokines-mil/configs/default.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = cfg[\"dynamics\"][\"random_seeds\"][0]\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full manifest entries: 10920\n",
      "HVGs: 4000\n"
     ]
    }
   ],
   "source": [
    "MANIFEST_PATH = cfg[\"data\"][\"manifest_path\"]\n",
    "\n",
    "with open(MANIFEST_PATH) as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "# Load HVG list (saved by preprocess_tubes.ipynb)\n",
    "HVG_PATH = str(Path(MANIFEST_PATH).parent / \"hvg_list.json\")\n",
    "with open(HVG_PATH) as f:\n",
    "    gene_names = json.load(f)\n",
    "\n",
    "print(f\"Full manifest entries: {len(manifest)}\")\n",
    "print(f\"HVGs: {len(gene_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All selected cytokines found in manifest.\n",
      "Selected 10 cytokines: ['IL-4', 'M-CSF', 'IL-10', 'TNF-alpha', 'IL-2', 'IL-22', 'VEGF', 'IL-12', 'OSM', 'HGF']\n"
     ]
    }
   ],
   "source": [
    "# --- Cytokine subset selection ---\n",
    "# Verify that these names exactly match the cytokine field in your manifest.json.\n",
    "# Run the cell below to cross-check before proceeding.\n",
    "# NOTE: TNF-alpha naming may vary (e.g. \"TNF-alpha\", \"TNFa\", \"TNF\") — adjust if flagged.\n",
    "\n",
    "# 5 cytokines with direct, PBMC-specific, strong effects — expected easy to learn\n",
    "EASY_CYTOKINES = [\n",
    "    \"IL-4\",        # #1 in preliminary run — strong Th2/B cell axis via STAT6\n",
    "    \"M-CSF\",       # #3 in preliminary run — direct monocyte survival/activation via CSF1R\n",
    "    \"IL-10\",       # #4 in preliminary run — strong monocyte/B cell STAT3 response\n",
    "    \"TNF-alpha\",   # canonical NF-κB activator — rapid direct effect on monocytes/T cells\n",
    "    \"IL-2\",        # #10 in preliminary run — direct T/NK activation via γc receptor\n",
    "]\n",
    "\n",
    "# 5 cytokines with indirect, non-PBMC-primary, or cascade-dependent effects — expected hard to learn\n",
    "HARD_CYTOKINES = [\n",
    "    \"IL-22\",   # targets epithelium via IL-22R1 — minimal direct PBMC transcriptional signal\n",
    "    \"VEGF\",    # endothelial target via VEGFR — PBMCs lack significant VEGFR expression\n",
    "    \"IL-12\",   # cascade control: drives IFN-γ in NK/T cells indirectly (Fig. 4i in source paper)\n",
    "    \"OSM\",     # Oncostatin M — non-PBMC primary targets, near bottom in preliminary run\n",
    "    \"HGF\",     # hepatocyte/stromal target via c-Met — weak direct PBMC response\n",
    "]\n",
    "\n",
    "SUBSET_CYTOKINES = EASY_CYTOKINES + HARD_CYTOKINES\n",
    "\n",
    "# Verify names against manifest before continuing\n",
    "manifest_cytokines = {e[\"cytokine\"] for e in manifest}\n",
    "missing = [c for c in SUBSET_CYTOKINES if c not in manifest_cytokines]\n",
    "if missing:\n",
    "    print(f\"WARNING — cytokines not found in manifest (check naming): {missing}\")\n",
    "    print(f\"Available names (sample): {sorted(manifest_cytokines)[:20]}\")\n",
    "else:\n",
    "    print(\"All selected cytokines found in manifest.\")\n",
    "print(f\"Selected {len(SUBSET_CYTOKINES)} cytokines: {SUBSET_CYTOKINES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset manifest entries: 1320\n",
      "Unique classes: ['HGF', 'IL-10', 'IL-12', 'IL-2', 'IL-22', 'IL-4', 'M-CSF', 'OSM', 'PBS', 'TNF-alpha', 'VEGF']\n",
      "n_classes = 11  (10 cytokines + PBS)\n"
     ]
    }
   ],
   "source": [
    "# Filter manifest to the 10-cytokine subset + PBS\n",
    "subset_manifest = filter_manifest(manifest, cytokines=SUBSET_CYTOKINES, include_pbs=True)\n",
    "\n",
    "# Count classes: 10 cytokines + PBS = 11\n",
    "subset_cytokine_names = {e[\"cytokine\"] for e in subset_manifest}\n",
    "print(f\"Subset manifest entries: {len(subset_manifest)}\")\n",
    "print(f\"Unique classes: {sorted(subset_cytokine_names)}\")\n",
    "print(f\"n_classes = {len(subset_cytokine_names)}  (10 cytokines + PBS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 91 (PBS at index 90)\n"
     ]
    }
   ],
   "source": [
    "# Label encoder — fitted on subset manifest for consistent index mapping\n",
    "LABEL_ENCODER_PATH = str(Path(MANIFEST_PATH).parent / \"label_encoder_subset.json\")\n",
    "label_encoder = CytokineLabel().fit(subset_manifest)\n",
    "label_encoder.save(LABEL_ENCODER_PATH)\n",
    "print(f\"Classes: {label_encoder.n_classes()} (PBS at index {label_encoder.encode('PBS')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_donors'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Donor-level train/val split (see CLAUDE.md Section 16) ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# D2 and D3 are the most biologically distinct donors in the cohort.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# They are held out for generalization testing and never used in optimizer steps.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m VAL_DONORS = \u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_donors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# [\"Donor2\", \"Donor3\"]\u001b[39;00m\n\u001b[32m      5\u001b[39m train_manifest, val_manifest = split_manifest_by_donor(subset_manifest, val_donors=VAL_DONORS)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain donors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m({e[\u001b[33m'\u001b[39m\u001b[33mdonor\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39me\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtrain_manifest})\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_manifest)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tubes)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'val_donors'"
     ]
    }
   ],
   "source": [
    "# --- Donor-level train/val split (see CLAUDE.md Section 16) ---\n",
    "# D2 and D3 are the most biologically distinct donors in the cohort.\n",
    "# They are held out for generalization testing and never used in optimizer steps.\n",
    "VAL_DONORS = cfg[\"data\"][\"val_donors\"]  # [\"Donor2\", \"Donor3\"]\n",
    "train_manifest, val_manifest = split_manifest_by_donor(subset_manifest, val_donors=VAL_DONORS)\n",
    "print(f\"Train donors: {sorted({e['donor'] for e in train_manifest})}  ({len(train_manifest)} tubes)\")\n",
    "print(f\"Val donors:   {sorted({e['donor'] for e in val_manifest})}  ({len(val_manifest)} tubes)\")\n",
    "\n",
    "# Save manifests for dataset construction\n",
    "SUBSET_MANIFEST_PATH = str(Path(MANIFEST_PATH).parent / \"manifest_subset.json\")\n",
    "TRAIN_MANIFEST_PATH  = str(Path(MANIFEST_PATH).parent / \"manifest_subset_train.json\")\n",
    "VAL_MANIFEST_PATH    = str(Path(MANIFEST_PATH).parent / \"manifest_subset_val.json\")\n",
    "with open(SUBSET_MANIFEST_PATH, \"w\") as f:\n",
    "    json.dump(subset_manifest, f)\n",
    "with open(TRAIN_MANIFEST_PATH, \"w\") as f:\n",
    "    json.dump(train_manifest, f)\n",
    "with open(VAL_MANIFEST_PATH, \"w\") as f:\n",
    "    json.dump(val_manifest, f)\n",
    "\n",
    "# Pseudo-tube datasets (Stage 2/3) — train and val separate\n",
    "# preload=True: loads all tubes as sparse matrices at init — eliminates disk I/O during training.\n",
    "train_tube_dataset = PseudoTubeDataset(TRAIN_MANIFEST_PATH, label_encoder, gene_names=gene_names, preload=True)\n",
    "val_tube_dataset   = PseudoTubeDataset(VAL_MANIFEST_PATH,   label_encoder, gene_names=gene_names, preload=True)\n",
    "print(f\"Train tubes: {len(train_tube_dataset)}\")\n",
    "print(f\"Val tubes:   {len(val_tube_dataset)}\")\n",
    "\n",
    "# --- Stage 1 manifest: one tube per cytokine, rotating donors (train donors only) ---\n",
    "STAGE1_MANIFEST_PATH = str(Path(MANIFEST_PATH).parent / \"manifest_stage1_subset.json\")\n",
    "_stage1_manifest = build_stage1_manifest(train_manifest, save_path=STAGE1_MANIFEST_PATH)\n",
    "\n",
    "# preload=True: loads all cells at init → in-memory shuffling, no disk I/O per batch\n",
    "cell_dataset = CellDataset(STAGE1_MANIFEST_PATH, gene_names=gene_names, preload=True)\n",
    "print(f\"Cells: {len(cell_dataset)}\")\n",
    "print(f\"Cell types: {cell_dataset.n_cell_types()}\")\n",
    "print(f\"NaN in X: {np.isnan(cell_dataset._X).any()}\")\n",
    "print(f\"Inf in X: {np.isinf(cell_dataset._X).any()}\")\n",
    "print(f\"X range: [{cell_dataset._X.min():.3f}, {cell_dataset._X.max():.3f}]\")\n",
    "\n",
    "cell_loader = DataLoader(cell_dataset, batch_size=256, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2. Stage 1 — Encoder Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   1/10 | loss=2.2185 | acc=0.3643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   2/10 | loss=0.5152 | acc=0.8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   3/10 | loss=0.3058 | acc=0.8871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   4/10 | loss=0.2230 | acc=0.9161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   5/10 | loss=0.1495 | acc=0.9582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   6/10 | loss=0.0946 | acc=0.9779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   7/10 | loss=0.0547 | acc=0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   8/10 | loss=0.0269 | acc=0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   9/10 | loss=0.0146 | acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch  10/10 | loss=0.0104 | acc=0.9988\n",
      "Encoder saved.\n"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder(\n",
    "    n_input_genes=len(gene_names),\n",
    "    n_cell_types=cell_dataset.n_cell_types(),\n",
    "    embed_dim=cfg[\"model\"][\"embedding_dim\"],\n",
    ")\n",
    "\n",
    "encoder = train_encoder(\n",
    "    encoder,\n",
    "    cell_loader,\n",
    "#    n_epochs=cfg[\"training\"][\"stage1_epochs\"],\n",
    "    n_epochs=10,\n",
    "    lr=cfg[\"training\"][\"lr\"],\n",
    "    momentum=cfg[\"training\"][\"momentum\"],\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "torch.save(encoder.state_dict(), \"encoder_stage1_subset.pt\")\n",
    "print(\"Encoder saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Stage 2 — MIL Training (encoder frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes derived from the label encoder — 11 for the 10-cytokine subset + PBS\n",
    "mil_model = build_mil_model(\n",
    "    encoder,\n",
    "    embed_dim=cfg[\"model\"][\"embedding_dim\"],\n",
    "    attention_hidden_dim=cfg[\"model\"][\"attention_hidden_dim\"],\n",
    "    n_classes=label_encoder.n_classes(),\n",
    "    encoder_frozen=True,\n",
    ")\n",
    "\n",
    "dynamics_stage2 = train_mil(\n",
    "    mil_model,\n",
    "    train_tube_dataset,                              # train donors only\n",
    "    n_epochs=20,\n",
    "    #n_epochs=cfg[\"training\"][\"stage2_epochs\"],\n",
    "    lr=cfg[\"training\"][\"lr\"],\n",
    "    momentum=cfg[\"training\"][\"momentum\"],\n",
    "    lr_scheduler=cfg[\"training\"][\"lr_scheduler\"],\n",
    "    lr_warmup_epochs=cfg[\"training\"][\"lr_warmup_epochs\"],\n",
    "    log_every_n_epochs=cfg[\"dynamics\"][\"log_every_n_epochs\"],\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    verbose=True,\n",
    "    val_dataset=val_tube_dataset,                    # observer only — no gradient updates\n",
    ")\n",
    "\n",
    "torch.save(mil_model.state_dict(), \"mil_stage2_subset.pt\")\n",
    "print(\"Stage 2 model saved.\")\n",
    "print(f\"Train records: {len(dynamics_stage2['records'])}\")\n",
    "print(f\"Val records:   {len(dynamics_stage2['val_records'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Stage 3 — Joint Fine-tuning (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mil_model.unfreeze_encoder()\n",
    "\n",
    "dynamics_stage3 = train_mil(\n",
    "    mil_model,\n",
    "    train_tube_dataset,                              # train donors only\n",
    "    n_epochs=20,\n",
    "    # n_epochs=cfg[\"training\"][\"stage3_epochs\"],\n",
    "    lr=cfg[\"training\"][\"lr\"] * 0.1,                  # lower LR for fine-tuning\n",
    "    momentum=cfg[\"training\"][\"momentum\"],\n",
    "    log_every_n_epochs=cfg[\"dynamics\"][\"log_every_n_epochs\"],\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    verbose=True,\n",
    "    val_dataset=val_tube_dataset,                    # observer only — no gradient updates\n",
    ")\n",
    "\n",
    "torch.save(mil_model.state_dict(), \"mil_stage3_subset.pt\")\n",
    "print(\"Stage 3 model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 5. Dynamics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Stage 2 dynamics for primary analysis (encoder frozen = cleaner dynamics)\n",
    "donor_traj     = aggregate_to_donor_level(dynamics_stage2[\"records\"])\n",
    "val_donor_traj = aggregate_to_donor_level(dynamics_stage2[\"val_records\"])\n",
    "\n",
    "# Learnability ranking — train donors\n",
    "learnability_result = rank_cytokines_by_learnability(donor_traj, exclude=[\"PBS\"])\n",
    "ranking = learnability_result[\"ranking\"]\n",
    "\n",
    "# Learnability ranking — val donors (D2, D3)\n",
    "val_learnability_result = rank_cytokines_by_learnability(val_donor_traj, exclude=[\"PBS\"])\n",
    "val_ranking = val_learnability_result[\"ranking\"]\n",
    "val_auc_map  = {cyt: auc for cyt, auc in val_ranking}\n",
    "\n",
    "print(\"Cytokine learnability ranking — Stage 2\")\n",
    "print(f\"Metric: {learnability_result['metric_description']}\")\n",
    "print()\n",
    "print(f\"{'Rank':>4}  {'Cytokine':<20}  {'Train AUC':>10}  {'Val AUC':>8}  Group\")\n",
    "print(\"-\" * 62)\n",
    "for i, (cyt, auc) in enumerate(ranking, 1):\n",
    "    group = \"EASY\" if cyt in EASY_CYTOKINES else \"HARD\"\n",
    "    val_auc = val_auc_map.get(cyt, float(\"nan\"))\n",
    "    print(f\"  {i:2d}.  {cyt:<20}  {auc:>10.3f}  {val_auc:>8.3f}  {group}\")\n",
    "\n",
    "# Evaluate pre-registered predictions (train)\n",
    "top5 = [r[0] for r in ranking[:5]]\n",
    "bot5 = [r[0] for r in ranking[-5:]]\n",
    "easy_in_top5 = sum(c in EASY_CYTOKINES for c in top5)\n",
    "hard_in_bot5 = sum(c in HARD_CYTOKINES for c in bot5)\n",
    "il12_rank = next((i + 1 for i, (c, _) in enumerate(ranking) if c == \"IL-12\"), None)\n",
    "print()\n",
    "print(\"Pre-registered prediction check (train donors):\")\n",
    "print(f\"  Easy cytokines in top-5: {easy_in_top5}/5  {top5}\")\n",
    "print(f\"  Hard cytokines in bot-5: {hard_in_bot5}/5  {bot5}\")\n",
    "if il12_rank is not None:\n",
    "    print(f\"  IL-12 rank: {il12_rank}/{len(ranking)}  (expected low — indirect cascade via IFN-γ)\")\n",
    "\n",
    "# Train/val rank correlation\n",
    "train_order = [c for c, _ in ranking]\n",
    "val_order   = [c for c, _ in val_ranking]\n",
    "if set(train_order) == set(val_order):\n",
    "    val_ranks_aligned = [val_order.index(c) for c in train_order]\n",
    "    rho, pval = spearmanr(range(len(train_order)), val_ranks_aligned)\n",
    "    print()\n",
    "    print(f\"Train vs val rank correlation: Spearman rho = {rho:.3f}  (p={pval:.3f})\")\n",
    "    print(\"  rho > 0.7 → ranking generalizes to held-out donors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves — train (solid) and val (dotted) in the same color per cytokine.\n",
    "# A persistent train > val gap indicates the model is exploiting donor-specific expression.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "epochs = dynamics_stage2[\"logged_epochs\"]\n",
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "for ax, group, group_label in zip(\n",
    "    axes,\n",
    "    [EASY_CYTOKINES, HARD_CYTOKINES],\n",
    "    [\"Easy group (direct, PBMC-specific responses)\", \"Hard group (indirect / non-PBMC-primary)\"],\n",
    "):\n",
    "    for ci, cyt in enumerate(group):\n",
    "        color = colors[ci % len(colors)]\n",
    "        ls_mod = \"--\" if cyt == \"IL-12\" else \"-\"\n",
    "        if cyt in donor_traj:\n",
    "            train_mean = np.mean(list(donor_traj[cyt].values()), axis=0)\n",
    "            ax.plot(epochs, train_mean, color=color, linestyle=ls_mod,\n",
    "                    alpha=0.9, label=f\"{cyt} (train)\")\n",
    "        if cyt in val_donor_traj:\n",
    "            val_mean = np.mean(list(val_donor_traj[cyt].values()), axis=0)\n",
    "            ax.plot(epochs, val_mean, color=color, linestyle=\":\",\n",
    "                    alpha=0.55, label=f\"{cyt} (val)\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"P(Y_correct | t) — softmax probability of correct cytokine class\")\n",
    "    ax.set_title(group_label)\n",
    "    ax.legend(fontsize=7, ncol=2)\n",
    "\n",
    "axes[1].annotate(\n",
    "    \"Solid = train donors (10 donors)\\nDotted = val donors (D2, D3)\\nIL-12: dashed = cascade control (indirect via IFN-γ)\",\n",
    "    xy=(0.02, 0.05), xycoords=\"axes fraction\", fontsize=7, color=\"gray\",\n",
    ")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Stage 2 learning curves — 10-cytokine subset (train vs val donors)\\n\"\n",
    "    \"Metric: mean p_correct_trajectory(t), aggregated to donor level \"\n",
    "    \"(median per donor, mean across donors)\",\n",
    "    fontsize=9,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"learning_curves_subset_trainval.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention entropy summary — train and val\n",
    "entropy_result     = compute_cytokine_entropy_summary(dynamics_stage2[\"records\"])\n",
    "val_entropy_result = compute_cytokine_entropy_summary(dynamics_stage2[\"val_records\"])\n",
    "entropy_summary     = entropy_result[\"summary\"]\n",
    "val_entropy_summary = val_entropy_result[\"summary\"]\n",
    "\n",
    "# Sort by train mean entropy (low=focused, high=pleiotropic)\n",
    "entropy_sorted = sorted(entropy_summary.items(), key=lambda x: x[1][\"mean_entropy\"])\n",
    "\n",
    "print(\"Cytokine attention entropy summary\")\n",
    "print(f\"Metric: {entropy_result['metric_description']}\")\n",
    "print()\n",
    "print(f\"{'Cytokine':<20}  {'Train H':>10}  {'Val H':>8}  Group\")\n",
    "print(\"-\" * 50)\n",
    "for cyt, stats in entropy_sorted:\n",
    "    group = \"EASY\" if cyt in EASY_CYTOKINES else (\"HARD\" if cyt in HARD_CYTOKINES else \"PBS\")\n",
    "    val_h = val_entropy_summary.get(cyt, {}).get(\"mean_entropy\", float(\"nan\"))\n",
    "    print(f\"  {cyt:<20}  {stats['mean_entropy']:>10.3f}  {val_h:>8.3f}  {group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion entropy summary — train and val\n",
    "confusion_result = compute_confusion_entropy_summary(\n",
    "    dynamics_stage2[\"confusion_entropy_trajectory\"], exclude=[\"PBS\"]\n",
    ")\n",
    "val_confusion_result = compute_confusion_entropy_summary(\n",
    "    dynamics_stage2[\"val_confusion_entropy_trajectory\"], exclude=[\"PBS\"]\n",
    ")\n",
    "val_conf_map = {cyt: auc for cyt, auc in val_confusion_result[\"ranking\"]}\n",
    "\n",
    "print(\"Cytokine confusion entropy ranking\")\n",
    "print(f\"Metric: {confusion_result['metric_description']}\")\n",
    "print()\n",
    "print(f\"{'Cytokine':<20}  {'Train AUC(H_c)':>14}  {'Val AUC(H_c)':>12}  Group\")\n",
    "print(\"-\" * 62)\n",
    "for cyt, auc in confusion_result[\"ranking\"]:\n",
    "    group = \"EASY\" if cyt in EASY_CYTOKINES else (\"HARD\" if cyt in HARD_CYTOKINES else \"PBS\")\n",
    "    val_auc = val_conf_map.get(cyt, float(\"nan\"))\n",
    "    print(f\"  {cyt:<20}  {auc:>14.3f}  {val_auc:>12.3f}  {group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 6. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1mopefvu9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Donor-level generalization check (see CLAUDE.md Section 16) ---\n",
    "# Val donors D2 and D3 were selected as the most biologically distinct donors:\n",
    "#   D3: highest baseline ISG expression (worst-case IFN generalization test)\n",
    "#   D2: aberrant CD14 Mono baseline resembling IL-32-β-stimulated state\n",
    "# Neither donor contributed any gradient updates to the model.\n",
    "\n",
    "train_ranking_list = rank_cytokines_by_learnability(donor_traj,     exclude=[\"PBS\"])[\"ranking\"]\n",
    "val_ranking_list   = rank_cytokines_by_learnability(val_donor_traj, exclude=[\"PBS\"])[\"ranking\"]\n",
    "\n",
    "train_order = [c for c, _ in train_ranking_list]\n",
    "val_order   = [c for c, _ in val_ranking_list]\n",
    "val_rank_by_cyt = {c: i for i, c in enumerate(val_order)}\n",
    "val_ranks_aligned = [val_rank_by_cyt[c] for c in train_order]\n",
    "\n",
    "rho, pval = spearmanr(range(len(train_order)), val_ranks_aligned)\n",
    "\n",
    "print(\"Donor-level generalization check — Stage 2\")\n",
    "print(f\"  Train donors: {sorted({e['donor'] for e in train_manifest})}\")\n",
    "print(f\"  Val donors:   {VAL_DONORS}  (never used in optimizer steps)\")\n",
    "print()\n",
    "print(f\"  Train/val rank correlation: Spearman rho = {rho:.3f}  (p={pval:.3f})\")\n",
    "print(f\"  Stable (rho > 0.7): {rho > 0.7}\")\n",
    "print()\n",
    "print(\"  Per-cytokine AUC (Train vs Val):\")\n",
    "print(f\"  {'Cytokine':<20}  {'Train AUC':>10}  {'Val AUC':>9}  {'Ratio V/T':>10}\")\n",
    "print(\"  \" + \"-\" * 56)\n",
    "val_auc_map2 = {c: a for c, a in val_ranking_list}\n",
    "for cyt, train_auc in train_ranking_list:\n",
    "    val_auc = val_auc_map2.get(cyt, float(\"nan\"))\n",
    "    ratio = val_auc / train_auc if train_auc > 0 else float(\"nan\")\n",
    "    flag = \"  ← possible overfit\" if ratio < 0.6 else \"\"\n",
    "    print(f\"  {cyt:<20}  {train_auc:>10.3f}  {val_auc:>9.3f}  {ratio:>10.2f}{flag}\")\n",
    "print()\n",
    "print(\"  Interpretation:\")\n",
    "print(\"    V/T ratio ≈ 1.0 → cytokine program generalizes to held-out donors.\")\n",
    "print(\"    V/T ratio << 1  → model may be exploiting donor-specific expression patterns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with multiple seeds to assess stability. See config random_seeds.\n"
     ]
    }
   ],
   "source": [
    "# Seed stability — run with all three seeds from config\n",
    "# NOTE: Pre-register your directional predictions BEFORE looking at these results.\n",
    "\n",
    "all_dynamics = [dynamics_stage2]  # Add dynamics from other seeds here\n",
    "\n",
    "# Example: to run with additional seeds, re-run train_mil with seed=123 and seed=7\n",
    "# and append to all_dynamics.\n",
    "\n",
    "if len(all_dynamics) > 1:\n",
    "    stability = check_seed_stability(all_dynamics, exclude=[\"PBS\"])\n",
    "    print(f\"Mean Spearman rho across seeds: {stability['mean_rho']:.3f}\")\n",
    "    print(f\"Stable ordering: {stability['stable']}\")\n",
    "else:\n",
    "    print(\"Run with multiple seeds to assess stability. See config random_seeds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pbmc_direct_activators:\n",
      "  members_found: ['IL-4', 'IL-10', 'IL-2']\n",
      "  within_auc_std: 1.4184111192825473\n",
      "  between_auc_std: 1.8113104055571458\n",
      "  passes: True\n",
      "\n",
      "non_pbmc_targets:\n",
      "  members_found: ['VEGF', 'HGF', 'IL-22']\n",
      "  within_auc_std: 0.7675078443251654\n",
      "  between_auc_std: 1.9040178552449398\n",
      "  passes: True\n"
     ]
    }
   ],
   "source": [
    "# Known functional groupings\n",
    "# IL-4, IL-10, IL-2 all signal through PBMC-expressed receptors — expected similar learnability.\n",
    "# VEGF, HGF, IL-22 all target non-PBMC cell types — expected similarly low learnability.\n",
    "# IL-12 is deliberately excluded from the hard-group clustering check: it is a cascade\n",
    "# control and its biological interpretation requires the learnability ranking to be read first.\n",
    "known_groups = {\n",
    "    \"pbmc_direct_activators\": [\"IL-4\", \"IL-10\", \"IL-2\"],\n",
    "    \"non_pbmc_targets\": [\"VEGF\", \"HGF\", \"IL-22\"],\n",
    "}\n",
    "\n",
    "grouping_result = check_functional_groupings(donor_traj, known_groups)\n",
    "for group, result in grouping_result.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    for k, v in result.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2 vs Stage 3 ranking correlation — train and val\n",
    "donor_traj_s3     = aggregate_to_donor_level(dynamics_stage3[\"records\"])\n",
    "val_donor_traj_s3 = aggregate_to_donor_level(dynamics_stage3[\"val_records\"])\n",
    "\n",
    "ranking_s3     = rank_cytokines_by_learnability(donor_traj_s3,     exclude=[\"PBS\"])\n",
    "val_ranking_s3 = rank_cytokines_by_learnability(val_donor_traj_s3, exclude=[\"PBS\"])\n",
    "\n",
    "stability_s2_s3 = check_seed_stability(\n",
    "    [dynamics_stage2, dynamics_stage3], exclude=[\"PBS\"]\n",
    ")\n",
    "print(\"Stage 2 vs Stage 3 ranking correlation\")\n",
    "print(\n",
    "    \"Metric: Spearman rho between cytokine learnability rankings \"\n",
    "    \"(AUC of donor-level p_correct_trajectory, median per donor, mean across donors)\"\n",
    ")\n",
    "print(f\"  Train: Spearman rho = {stability_s2_s3['mean_rho']:.3f}\")\n",
    "print(f\"  Stable across stages (rho > 0.7): {stability_s2_s3['stable']}\")\n",
    "\n",
    "# Val correlation across stages\n",
    "val_s2_order = [c for c, _ in val_learnability_result[\"ranking\"]]\n",
    "val_s3_order = [c for c, _ in val_ranking_s3[\"ranking\"]]\n",
    "if set(val_s2_order) == set(val_s3_order):\n",
    "    val_s3_aligned = [val_s3_order.index(c) for c in val_s2_order]\n",
    "    rho_val, _ = spearmanr(range(len(val_s2_order)), val_s3_aligned)\n",
    "    print(f\"  Val:   Spearman rho = {rho_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198010bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (biovenv)",
   "language": "python",
   "name": "biovenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
