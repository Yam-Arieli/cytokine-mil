{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Cytokine Signaling Dynamics — 10-Cytokine Subset Experiment\n",
    "\n",
    "Runs the full AB-MIL pipeline on a curated 10-cytokine subset + PBS control.\n",
    "\n",
    "## Rationale for cytokine selection\n",
    "\n",
    "The subset is designed to provide a **clear expected contrast** between easy-to-learn\n",
    "(high learnability AUC) and hard-to-learn (low AUC) cytokines, grounded in both\n",
    "the literature and preliminary full-experiment rankings reported in the source paper.\n",
    "\n",
    "The hard group includes IL-12 as a **cascade positive control**: the paper (Fig. 4i)\n",
    "explicitly shows that IL-12 drives its PBMC effects indirectly via IFN-γ induction,\n",
    "making it a clean test of whether the model recovers secondary-cascade cytokines as\n",
    "harder to learn than direct activators.\n",
    "\n",
    "### Expected easy group — direct, PBMC-specific, strong 24-h transcriptional response\n",
    "| Cytokine | Key mechanism | Evidence |\n",
    "|---|---|---|\n",
    "| IL-4 | Strong Th2/B cell axis via STAT6 | Ranked #1 in full-experiment preliminary run |\n",
    "| M-CSF | Direct monocyte survival/activation via CSF1R | Ranked #3 in preliminary run |\n",
    "| IL-10 | Strong monocyte/B cell response via STAT3 | Ranked #4 in preliminary run |\n",
    "| TNF-alpha | Canonical early NF-κB in monocytes/T cells | Well-established rapid inflammatory activator |\n",
    "| IL-2 | Direct T/NK activation via γc receptor | Ranked #10 in preliminary run |\n",
    "\n",
    "### Expected hard group — indirect, non-PBMC targets, or cascade-dependent\n",
    "| Cytokine | Key mechanism | Why hard |\n",
    "|---|---|---|\n",
    "| IL-22 | Epithelial STAT3 via IL-22R1 | Targets epithelium, not PBMCs — minimal direct PBMC signal |\n",
    "| VEGF | Endothelial growth via VEGFR | Endothelial target; PBMCs lack significant VEGFR expression |\n",
    "| IL-12 | IFN-γ induction in NK/T cells | **Cascade control**: effect is indirect (Fig. 4i explicitly shows secondary IFN-γ cascade) |\n",
    "| OSM | Oncostatin M via gp130 | Non-PBMC primary targets; ranked near bottom in preliminary run |\n",
    "| HGF | Hepatocyte growth via c-Met | Hepatocyte/stromal target; minimal direct PBMC transcriptional response |\n",
    "\n",
    "**Pre-registered predictions:**\n",
    "- IL-4, M-CSF, IL-10 should rank in the top 3 by learnability AUC.\n",
    "- IL-22, VEGF, HGF should rank in the bottom 3.\n",
    "- IL-12 should rank low despite being immunostimulatory (indirect cascade effect, not direct PBMC activation).\n",
    "\n",
    "Connect to the cluster kernel before running.\n",
    "All paths in `configs/default.yaml` point to cluster storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport yaml\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom scipy.stats import spearmanr\nfrom torch.utils.data import DataLoader\n\nfrom cytokine_mil.data.label_encoder import CytokineLabel\nfrom cytokine_mil.data.dataset import PseudoTubeDataset, CellDataset\nfrom cytokine_mil.models.instance_encoder import InstanceEncoder\nfrom cytokine_mil.models.attention import AttentionModule\nfrom cytokine_mil.models.bag_classifier import BagClassifier\nfrom cytokine_mil.models.cytokine_abmil import CytokineABMIL\nfrom cytokine_mil.training.train_encoder import train_encoder\nfrom cytokine_mil.training.train_mil import train_mil\nfrom cytokine_mil.experiment_setup import (\n    build_stage1_manifest,\n    filter_manifest,\n    split_manifest_by_donor,\n    build_encoder,\n    build_mil_model,\n)\nfrom cytokine_mil.analysis.dynamics import (\n    aggregate_to_donor_level,\n    rank_cytokines_by_learnability,\n    compute_cytokine_entropy_summary,\n    compute_confusion_entropy_summary,\n    build_cell_type_confidence_matrix,\n)\nfrom cytokine_mil.analysis.validation import (\n    check_seed_stability,\n    check_functional_groupings,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# --- Config ---\n",
    "with open(\"cytokines/cytokines-mil/configs/default.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = cfg[\"dynamics\"][\"random_seeds\"][0]\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full manifest entries: 10920\n",
      "HVGs: 4000\n"
     ]
    }
   ],
   "source": [
    "MANIFEST_PATH = cfg[\"data\"][\"manifest_path\"]\n",
    "\n",
    "with open(MANIFEST_PATH) as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "# Load HVG list (saved by preprocess_tubes.ipynb)\n",
    "HVG_PATH = str(Path(MANIFEST_PATH).parent / \"hvg_list.json\")\n",
    "with open(HVG_PATH) as f:\n",
    "    gene_names = json.load(f)\n",
    "\n",
    "print(f\"Full manifest entries: {len(manifest)}\")\n",
    "print(f\"HVGs: {len(gene_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All selected cytokines found in manifest.\n",
      "Selected 10 cytokines: ['IL-4', 'M-CSF', 'IL-10', 'TNF-alpha', 'IL-2', 'IL-22', 'VEGF', 'IL-12', 'OSM', 'HGF']\n"
     ]
    }
   ],
   "source": [
    "# --- Cytokine subset selection ---\n",
    "# Verify that these names exactly match the cytokine field in your manifest.json.\n",
    "# Run the cell below to cross-check before proceeding.\n",
    "# NOTE: TNF-alpha naming may vary (e.g. \"TNF-alpha\", \"TNFa\", \"TNF\") — adjust if flagged.\n",
    "\n",
    "# 5 cytokines with direct, PBMC-specific, strong effects — expected easy to learn\n",
    "EASY_CYTOKINES = [\n",
    "    \"IL-4\",        # #1 in preliminary run — strong Th2/B cell axis via STAT6\n",
    "    \"M-CSF\",       # #3 in preliminary run — direct monocyte survival/activation via CSF1R\n",
    "    \"IL-10\",       # #4 in preliminary run — strong monocyte/B cell STAT3 response\n",
    "    \"TNF-alpha\",   # canonical NF-κB activator — rapid direct effect on monocytes/T cells\n",
    "    \"IL-2\",        # #10 in preliminary run — direct T/NK activation via γc receptor\n",
    "]\n",
    "\n",
    "# 5 cytokines with indirect, non-PBMC-primary, or cascade-dependent effects — expected hard to learn\n",
    "HARD_CYTOKINES = [\n",
    "    \"IL-22\",   # targets epithelium via IL-22R1 — minimal direct PBMC transcriptional signal\n",
    "    \"VEGF\",    # endothelial target via VEGFR — PBMCs lack significant VEGFR expression\n",
    "    \"IL-12\",   # cascade control: drives IFN-γ in NK/T cells indirectly (Fig. 4i in source paper)\n",
    "    \"OSM\",     # Oncostatin M — non-PBMC primary targets, near bottom in preliminary run\n",
    "    \"HGF\",     # hepatocyte/stromal target via c-Met — weak direct PBMC response\n",
    "]\n",
    "\n",
    "SUBSET_CYTOKINES = EASY_CYTOKINES + HARD_CYTOKINES\n",
    "\n",
    "# Verify names against manifest before continuing\n",
    "manifest_cytokines = {e[\"cytokine\"] for e in manifest}\n",
    "missing = [c for c in SUBSET_CYTOKINES if c not in manifest_cytokines]\n",
    "if missing:\n",
    "    print(f\"WARNING — cytokines not found in manifest (check naming): {missing}\")\n",
    "    print(f\"Available names (sample): {sorted(manifest_cytokines)[:20]}\")\n",
    "else:\n",
    "    print(\"All selected cytokines found in manifest.\")\n",
    "print(f\"Selected {len(SUBSET_CYTOKINES)} cytokines: {SUBSET_CYTOKINES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset manifest entries: 1320\n",
      "Unique classes: ['HGF', 'IL-10', 'IL-12', 'IL-2', 'IL-22', 'IL-4', 'M-CSF', 'OSM', 'PBS', 'TNF-alpha', 'VEGF']\n",
      "n_classes = 11  (10 cytokines + PBS)\n"
     ]
    }
   ],
   "source": [
    "# Filter manifest to the 10-cytokine subset + PBS\n",
    "subset_manifest = filter_manifest(manifest, cytokines=SUBSET_CYTOKINES, include_pbs=True)\n",
    "\n",
    "# Count classes: 10 cytokines + PBS = 11\n",
    "subset_cytokine_names = {e[\"cytokine\"] for e in subset_manifest}\n",
    "print(f\"Subset manifest entries: {len(subset_manifest)}\")\n",
    "print(f\"Unique classes: {sorted(subset_cytokine_names)}\")\n",
    "print(f\"n_classes = {len(subset_cytokine_names)}  (10 cytokines + PBS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 91 (PBS at index 90)\n"
     ]
    }
   ],
   "source": [
    "# Label encoder — fitted on subset manifest for consistent index mapping\n",
    "LABEL_ENCODER_PATH = str(Path(MANIFEST_PATH).parent / \"label_encoder_subset.json\")\n",
    "label_encoder = CytokineLabel().fit(subset_manifest)\n",
    "label_encoder.save(LABEL_ENCODER_PATH)\n",
    "print(f\"Classes: {label_encoder.n_classes()} (PBS at index {label_encoder.encode('PBS')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# --- Donor-level train/val split (see CLAUDE.md Section 16) ---\n# D2 and D3 are the most biologically distinct donors in the cohort.\n# They are held out for generalization testing and never used in optimizer steps.\nVAL_DONORS = cfg[\"data\"][\"val_donors\"]  # [\"Donor2\", \"Donor3\"]\ntrain_manifest, val_manifest = split_manifest_by_donor(subset_manifest, val_donors=VAL_DONORS)\nprint(f\"Train donors: {sorted({e['donor'] for e in train_manifest})}  ({len(train_manifest)} tubes)\")\nprint(f\"Val donors:   {sorted({e['donor'] for e in val_manifest})}  ({len(val_manifest)} tubes)\")\n\n# Save manifests for dataset construction\nSUBSET_MANIFEST_PATH = str(Path(MANIFEST_PATH).parent / \"manifest_subset.json\")\nTRAIN_MANIFEST_PATH  = str(Path(MANIFEST_PATH).parent / \"manifest_subset_train.json\")\nVAL_MANIFEST_PATH    = str(Path(MANIFEST_PATH).parent / \"manifest_subset_val.json\")\nwith open(SUBSET_MANIFEST_PATH, \"w\") as f:\n    json.dump(subset_manifest, f)\nwith open(TRAIN_MANIFEST_PATH, \"w\") as f:\n    json.dump(train_manifest, f)\nwith open(VAL_MANIFEST_PATH, \"w\") as f:\n    json.dump(val_manifest, f)\n\n# Pseudo-tube datasets (Stage 2/3) — train and val separate\n# preload=True: loads all tubes as sparse matrices at init — eliminates disk I/O during training.\ntrain_tube_dataset = PseudoTubeDataset(TRAIN_MANIFEST_PATH, label_encoder, gene_names=gene_names, preload=True)\nval_tube_dataset   = PseudoTubeDataset(VAL_MANIFEST_PATH,   label_encoder, gene_names=gene_names, preload=True)\nprint(f\"Train tubes: {len(train_tube_dataset)}\")\nprint(f\"Val tubes:   {len(val_tube_dataset)}\")\n\n# --- Stage 1 manifest: one tube per cytokine, rotating donors (train donors only) ---\nSTAGE1_MANIFEST_PATH = str(Path(MANIFEST_PATH).parent / \"manifest_stage1_subset.json\")\n_stage1_manifest = build_stage1_manifest(train_manifest, save_path=STAGE1_MANIFEST_PATH)\n\n# preload=True: loads all cells at init → in-memory shuffling, no disk I/O per batch\ncell_dataset = CellDataset(STAGE1_MANIFEST_PATH, gene_names=gene_names, preload=True)\nprint(f\"Cells: {len(cell_dataset)}\")\nprint(f\"Cell types: {cell_dataset.n_cell_types()}\")\nprint(f\"NaN in X: {np.isnan(cell_dataset._X).any()}\")\nprint(f\"Inf in X: {np.isinf(cell_dataset._X).any()}\")\nprint(f\"X range: [{cell_dataset._X.min():.3f}, {cell_dataset._X.max():.3f}]\")\n\ncell_loader = DataLoader(cell_dataset, batch_size=256, shuffle=True, num_workers=0)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2. Stage 1 — Encoder Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   1/10 | loss=2.2185 | acc=0.3643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   2/10 | loss=0.5152 | acc=0.8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   3/10 | loss=0.3058 | acc=0.8871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   4/10 | loss=0.2230 | acc=0.9161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   5/10 | loss=0.1495 | acc=0.9582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   6/10 | loss=0.0946 | acc=0.9779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   7/10 | loss=0.0547 | acc=0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   8/10 | loss=0.0269 | acc=0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch   9/10 | loss=0.0146 | acc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch  10/10 | loss=0.0104 | acc=0.9988\n",
      "Encoder saved.\n"
     ]
    }
   ],
   "source": [
    "encoder = build_encoder(\n",
    "    n_input_genes=len(gene_names),\n",
    "    n_cell_types=cell_dataset.n_cell_types(),\n",
    "    embed_dim=cfg[\"model\"][\"embedding_dim\"],\n",
    ")\n",
    "\n",
    "encoder = train_encoder(\n",
    "    encoder,\n",
    "    cell_loader,\n",
    "#    n_epochs=cfg[\"training\"][\"stage1_epochs\"],\n",
    "    n_epochs=10,\n",
    "    lr=cfg[\"training\"][\"lr\"],\n",
    "    momentum=cfg[\"training\"][\"momentum\"],\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "torch.save(encoder.state_dict(), \"encoder_stage1_subset.pt\")\n",
    "print(\"Encoder saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Stage 2 — MIL Training (encoder frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# n_classes derived from the label encoder — 11 for the 10-cytokine subset + PBS\nmil_model = build_mil_model(\n    encoder,\n    embed_dim=cfg[\"model\"][\"embedding_dim\"],\n    attention_hidden_dim=cfg[\"model\"][\"attention_hidden_dim\"],\n    n_classes=label_encoder.n_classes(),\n    encoder_frozen=True,\n)\n\ndynamics_stage2 = train_mil(\n    mil_model,\n    train_tube_dataset,                              # train donors only\n    n_epochs=20,\n    #n_epochs=cfg[\"training\"][\"stage2_epochs\"],\n    lr=cfg[\"training\"][\"lr\"],\n    momentum=cfg[\"training\"][\"momentum\"],\n    lr_scheduler=cfg[\"training\"][\"lr_scheduler\"],\n    lr_warmup_epochs=cfg[\"training\"][\"lr_warmup_epochs\"],\n    log_every_n_epochs=cfg[\"dynamics\"][\"log_every_n_epochs\"],\n    device=DEVICE,\n    seed=SEED,\n    verbose=True,\n    val_dataset=val_tube_dataset,                    # observer only — no gradient updates\n)\n\ntorch.save(mil_model.state_dict(), \"mil_stage2_subset.pt\")\nprint(\"Stage 2 model saved.\")\nprint(f\"Train records: {len(dynamics_stage2['records'])}\")\nprint(f\"Val records:   {len(dynamics_stage2['val_records'])}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Stage 3 — Joint Fine-tuning (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "mil_model.unfreeze_encoder()\n\ndynamics_stage3 = train_mil(\n    mil_model,\n    train_tube_dataset,                              # train donors only\n    n_epochs=20,\n    # n_epochs=cfg[\"training\"][\"stage3_epochs\"],\n    lr=cfg[\"training\"][\"lr\"] * 0.1,                  # lower LR for fine-tuning\n    momentum=cfg[\"training\"][\"momentum\"],\n    log_every_n_epochs=cfg[\"dynamics\"][\"log_every_n_epochs\"],\n    device=DEVICE,\n    seed=SEED,\n    verbose=True,\n    val_dataset=val_tube_dataset,                    # observer only — no gradient updates\n)\n\ntorch.save(mil_model.state_dict(), \"mil_stage3_subset.pt\")\nprint(\"Stage 3 model saved.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 5. Dynamics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# Use Stage 2 dynamics for primary analysis (encoder frozen = cleaner dynamics)\ndonor_traj     = aggregate_to_donor_level(dynamics_stage2[\"records\"])\nval_donor_traj = aggregate_to_donor_level(dynamics_stage2[\"val_records\"])\n\n# Learnability ranking — train donors\nlearnability_result = rank_cytokines_by_learnability(donor_traj, exclude=[\"PBS\"])\nranking = learnability_result[\"ranking\"]\n\n# Learnability ranking — val donors (D2, D3)\nval_learnability_result = rank_cytokines_by_learnability(val_donor_traj, exclude=[\"PBS\"])\nval_ranking = val_learnability_result[\"ranking\"]\nval_auc_map  = {cyt: auc for cyt, auc in val_ranking}\n\nprint(\"Cytokine learnability ranking — Stage 2\")\nprint(f\"Metric: {learnability_result['metric_description']}\")\nprint()\nprint(f\"{'Rank':>4}  {'Cytokine':<20}  {'Train AUC':>10}  {'Val AUC':>8}  Group\")\nprint(\"-\" * 62)\nfor i, (cyt, auc) in enumerate(ranking, 1):\n    group = \"EASY\" if cyt in EASY_CYTOKINES else \"HARD\"\n    val_auc = val_auc_map.get(cyt, float(\"nan\"))\n    print(f\"  {i:2d}.  {cyt:<20}  {auc:>10.3f}  {val_auc:>8.3f}  {group}\")\n\n# Evaluate pre-registered predictions (train)\ntop5 = [r[0] for r in ranking[:5]]\nbot5 = [r[0] for r in ranking[-5:]]\neasy_in_top5 = sum(c in EASY_CYTOKINES for c in top5)\nhard_in_bot5 = sum(c in HARD_CYTOKINES for c in bot5)\nil12_rank = next((i + 1 for i, (c, _) in enumerate(ranking) if c == \"IL-12\"), None)\nprint()\nprint(\"Pre-registered prediction check (train donors):\")\nprint(f\"  Easy cytokines in top-5: {easy_in_top5}/5  {top5}\")\nprint(f\"  Hard cytokines in bot-5: {hard_in_bot5}/5  {bot5}\")\nif il12_rank is not None:\n    print(f\"  IL-12 rank: {il12_rank}/{len(ranking)}  (expected low — indirect cascade via IFN-γ)\")\n\n# Train/val rank correlation\ntrain_order = [c for c, _ in ranking]\nval_order   = [c for c, _ in val_ranking]\nif set(train_order) == set(val_order):\n    val_ranks_aligned = [val_order.index(c) for c in train_order]\n    rho, pval = spearmanr(range(len(train_order)), val_ranks_aligned)\n    print()\n    print(f\"Train vs val rank correlation: Spearman rho = {rho:.3f}  (p={pval:.3f})\")\n    print(\"  rho > 0.7 → ranking generalizes to held-out donors.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Plot learning curves — train (solid) and val (dotted) in the same color per cytokine.\n# A persistent train > val gap indicates the model is exploiting donor-specific expression.\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\nepochs = dynamics_stage2[\"logged_epochs\"]\ncolors = plt.cm.tab10.colors\n\nfor ax, group, group_label in zip(\n    axes,\n    [EASY_CYTOKINES, HARD_CYTOKINES],\n    [\"Easy group (direct, PBMC-specific responses)\", \"Hard group (indirect / non-PBMC-primary)\"],\n):\n    for ci, cyt in enumerate(group):\n        color = colors[ci % len(colors)]\n        ls_mod = \"--\" if cyt == \"IL-12\" else \"-\"\n        if cyt in donor_traj:\n            train_mean = np.mean(list(donor_traj[cyt].values()), axis=0)\n            ax.plot(epochs, train_mean, color=color, linestyle=ls_mod,\n                    alpha=0.9, label=f\"{cyt} (train)\")\n        if cyt in val_donor_traj:\n            val_mean = np.mean(list(val_donor_traj[cyt].values()), axis=0)\n            ax.plot(epochs, val_mean, color=color, linestyle=\":\",\n                    alpha=0.55, label=f\"{cyt} (val)\")\n    ax.set_xlabel(\"Epoch\")\n    ax.set_ylabel(\"P(Y_correct | t) — softmax probability of correct cytokine class\")\n    ax.set_title(group_label)\n    ax.legend(fontsize=7, ncol=2)\n\naxes[1].annotate(\n    \"Solid = train donors (10 donors)\\nDotted = val donors (D2, D3)\\nIL-12: dashed = cascade control (indirect via IFN-γ)\",\n    xy=(0.02, 0.05), xycoords=\"axes fraction\", fontsize=7, color=\"gray\",\n)\n\nplt.suptitle(\n    \"Stage 2 learning curves — 10-cytokine subset (train vs val donors)\\n\"\n    \"Metric: mean p_correct_trajectory(t), aggregated to donor level \"\n    \"(median per donor, mean across donors)\",\n    fontsize=9,\n)\nplt.tight_layout()\nplt.savefig(\"learning_curves_subset_trainval.png\", dpi=150)\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Attention entropy summary — train and val\nentropy_result     = compute_cytokine_entropy_summary(dynamics_stage2[\"records\"])\nval_entropy_result = compute_cytokine_entropy_summary(dynamics_stage2[\"val_records\"])\nentropy_summary     = entropy_result[\"summary\"]\nval_entropy_summary = val_entropy_result[\"summary\"]\n\n# Sort by train mean entropy (low=focused, high=pleiotropic)\nentropy_sorted = sorted(entropy_summary.items(), key=lambda x: x[1][\"mean_entropy\"])\n\nprint(\"Cytokine attention entropy summary\")\nprint(f\"Metric: {entropy_result['metric_description']}\")\nprint()\nprint(f\"{'Cytokine':<20}  {'Train H':>10}  {'Val H':>8}  Group\")\nprint(\"-\" * 50)\nfor cyt, stats in entropy_sorted:\n    group = \"EASY\" if cyt in EASY_CYTOKINES else (\"HARD\" if cyt in HARD_CYTOKINES else \"PBS\")\n    val_h = val_entropy_summary.get(cyt, {}).get(\"mean_entropy\", float(\"nan\"))\n    print(f\"  {cyt:<20}  {stats['mean_entropy']:>10.3f}  {val_h:>8.3f}  {group}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Confusion entropy summary — train and val\nconfusion_result = compute_confusion_entropy_summary(\n    dynamics_stage2[\"confusion_entropy_trajectory\"], exclude=[\"PBS\"]\n)\nval_confusion_result = compute_confusion_entropy_summary(\n    dynamics_stage2[\"val_confusion_entropy_trajectory\"], exclude=[\"PBS\"]\n)\nval_conf_map = {cyt: auc for cyt, auc in val_confusion_result[\"ranking\"]}\n\nprint(\"Cytokine confusion entropy ranking\")\nprint(f\"Metric: {confusion_result['metric_description']}\")\nprint()\nprint(f\"{'Cytokine':<20}  {'Train AUC(H_c)':>14}  {'Val AUC(H_c)':>12}  Group\")\nprint(\"-\" * 62)\nfor cyt, auc in confusion_result[\"ranking\"]:\n    group = \"EASY\" if cyt in EASY_CYTOKINES else (\"HARD\" if cyt in HARD_CYTOKINES else \"PBS\")\n    val_auc = val_conf_map.get(cyt, float(\"nan\"))\n    print(f\"  {cyt:<20}  {auc:>14.3f}  {val_auc:>12.3f}  {group}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 6. Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1mopefvu9b",
   "source": "# --- Donor-level generalization check (see CLAUDE.md Section 16) ---\n# Val donors D2 and D3 were selected as the most biologically distinct donors:\n#   D3: highest baseline ISG expression (worst-case IFN generalization test)\n#   D2: aberrant CD14 Mono baseline resembling IL-32-β-stimulated state\n# Neither donor contributed any gradient updates to the model.\n\ntrain_ranking_list = rank_cytokines_by_learnability(donor_traj,     exclude=[\"PBS\"])[\"ranking\"]\nval_ranking_list   = rank_cytokines_by_learnability(val_donor_traj, exclude=[\"PBS\"])[\"ranking\"]\n\ntrain_order = [c for c, _ in train_ranking_list]\nval_order   = [c for c, _ in val_ranking_list]\nval_rank_by_cyt = {c: i for i, c in enumerate(val_order)}\nval_ranks_aligned = [val_rank_by_cyt[c] for c in train_order]\n\nrho, pval = spearmanr(range(len(train_order)), val_ranks_aligned)\n\nprint(\"Donor-level generalization check — Stage 2\")\nprint(f\"  Train donors: {sorted({e['donor'] for e in train_manifest})}\")\nprint(f\"  Val donors:   {VAL_DONORS}  (never used in optimizer steps)\")\nprint()\nprint(f\"  Train/val rank correlation: Spearman rho = {rho:.3f}  (p={pval:.3f})\")\nprint(f\"  Stable (rho > 0.7): {rho > 0.7}\")\nprint()\nprint(\"  Per-cytokine AUC (Train vs Val):\")\nprint(f\"  {'Cytokine':<20}  {'Train AUC':>10}  {'Val AUC':>9}  {'Ratio V/T':>10}\")\nprint(\"  \" + \"-\" * 56)\nval_auc_map2 = {c: a for c, a in val_ranking_list}\nfor cyt, train_auc in train_ranking_list:\n    val_auc = val_auc_map2.get(cyt, float(\"nan\"))\n    ratio = val_auc / train_auc if train_auc > 0 else float(\"nan\")\n    flag = \"  ← possible overfit\" if ratio < 0.6 else \"\"\n    print(f\"  {cyt:<20}  {train_auc:>10.3f}  {val_auc:>9.3f}  {ratio:>10.2f}{flag}\")\nprint()\nprint(\"  Interpretation:\")\nprint(\"    V/T ratio ≈ 1.0 → cytokine program generalizes to held-out donors.\")\nprint(\"    V/T ratio << 1  → model may be exploiting donor-specific expression patterns.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run with multiple seeds to assess stability. See config random_seeds.\n"
     ]
    }
   ],
   "source": [
    "# Seed stability — run with all three seeds from config\n",
    "# NOTE: Pre-register your directional predictions BEFORE looking at these results.\n",
    "\n",
    "all_dynamics = [dynamics_stage2]  # Add dynamics from other seeds here\n",
    "\n",
    "# Example: to run with additional seeds, re-run train_mil with seed=123 and seed=7\n",
    "# and append to all_dynamics.\n",
    "\n",
    "if len(all_dynamics) > 1:\n",
    "    stability = check_seed_stability(all_dynamics, exclude=[\"PBS\"])\n",
    "    print(f\"Mean Spearman rho across seeds: {stability['mean_rho']:.3f}\")\n",
    "    print(f\"Stable ordering: {stability['stable']}\")\n",
    "else:\n",
    "    print(\"Run with multiple seeds to assess stability. See config random_seeds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pbmc_direct_activators:\n",
      "  members_found: ['IL-4', 'IL-10', 'IL-2']\n",
      "  within_auc_std: 1.4184111192825473\n",
      "  between_auc_std: 1.8113104055571458\n",
      "  passes: True\n",
      "\n",
      "non_pbmc_targets:\n",
      "  members_found: ['VEGF', 'HGF', 'IL-22']\n",
      "  within_auc_std: 0.7675078443251654\n",
      "  between_auc_std: 1.9040178552449398\n",
      "  passes: True\n"
     ]
    }
   ],
   "source": [
    "# Known functional groupings\n",
    "# IL-4, IL-10, IL-2 all signal through PBMC-expressed receptors — expected similar learnability.\n",
    "# VEGF, HGF, IL-22 all target non-PBMC cell types — expected similarly low learnability.\n",
    "# IL-12 is deliberately excluded from the hard-group clustering check: it is a cascade\n",
    "# control and its biological interpretation requires the learnability ranking to be read first.\n",
    "known_groups = {\n",
    "    \"pbmc_direct_activators\": [\"IL-4\", \"IL-10\", \"IL-2\"],\n",
    "    \"non_pbmc_targets\": [\"VEGF\", \"HGF\", \"IL-22\"],\n",
    "}\n",
    "\n",
    "grouping_result = check_functional_groupings(donor_traj, known_groups)\n",
    "for group, result in grouping_result.items():\n",
    "    print(f\"\\n{group}:\")\n",
    "    for k, v in result.items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Stage 2 vs Stage 3 ranking correlation — train and val\ndonor_traj_s3     = aggregate_to_donor_level(dynamics_stage3[\"records\"])\nval_donor_traj_s3 = aggregate_to_donor_level(dynamics_stage3[\"val_records\"])\n\nranking_s3     = rank_cytokines_by_learnability(donor_traj_s3,     exclude=[\"PBS\"])\nval_ranking_s3 = rank_cytokines_by_learnability(val_donor_traj_s3, exclude=[\"PBS\"])\n\nstability_s2_s3 = check_seed_stability(\n    [dynamics_stage2, dynamics_stage3], exclude=[\"PBS\"]\n)\nprint(\"Stage 2 vs Stage 3 ranking correlation\")\nprint(\n    \"Metric: Spearman rho between cytokine learnability rankings \"\n    \"(AUC of donor-level p_correct_trajectory, median per donor, mean across donors)\"\n)\nprint(f\"  Train: Spearman rho = {stability_s2_s3['mean_rho']:.3f}\")\nprint(f\"  Stable across stages (rho > 0.7): {stability_s2_s3['stable']}\")\n\n# Val correlation across stages\nval_s2_order = [c for c, _ in val_learnability_result[\"ranking\"]]\nval_s3_order = [c for c, _ in val_ranking_s3[\"ranking\"]]\nif set(val_s2_order) == set(val_s3_order):\n    val_s3_aligned = [val_s3_order.index(c) for c in val_s2_order]\n    rho_val, _ = spearmanr(range(len(val_s2_order)), val_s3_aligned)\n    print(f\"  Val:   Spearman rho = {rho_val:.3f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198010bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (biovenv)",
   "language": "python",
   "name": "biovenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}